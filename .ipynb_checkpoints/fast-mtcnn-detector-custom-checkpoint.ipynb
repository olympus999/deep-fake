{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-09T07:14:32.518Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import applications\n",
    "# # from keras.backend.tensorflow_backend import set_session\n",
    "# import tensorflow as tf\n",
    "# config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "# config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "# sess = tf.compat.v1.Session(config=config)\n",
    "# # set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.models import Model \n",
    "# from tensorflow.keras import optimizers\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.448123Z",
     "start_time": "2020-02-09T07:13:05.367095Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'facenet_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3e31975c18c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFileVideoStream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'facenet_pytorch'"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from imutils.video import FileVideoStream\n",
    "from custom_utils import return_all_video_paths\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import imutils\n",
    "import threading\n",
    "from IPython.display import display\n",
    "from os.path import join\n",
    "from tqdm.notebook import tqdm\n",
    "import traceback\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.450099Z",
     "start_time": "2020-02-09T07:13:05.368Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.451099Z",
     "start_time": "2020-02-09T07:13:05.372Z"
    }
   },
   "outputs": [],
   "source": [
    "class FastMTCNN(object):\n",
    "    \"\"\"Fast MTCNN implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, resize=1, *args, **kwargs):\n",
    "        \"\"\"Constructor for FastMTCNN class.\n",
    "        \n",
    "        Arguments:\n",
    "            stride (int): The detection stride. Faces will be detected every `stride` frames\n",
    "                and remembered for `stride-1` frames.\n",
    "        \n",
    "        Keyword arguments:\n",
    "            resize (float): Fractional frame scaling. [default: {1}]\n",
    "            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "        \"\"\"\n",
    "        self.resize = resize\n",
    "        self.mtcnn = MTCNN(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, frames, double_frames=False):\n",
    "        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n",
    "        frames_original = frames.copy()\n",
    "        if self.resize != 1:\n",
    "            frames = [f.resize([int(d * self.resize) for d in f.size]) for f in frames]\n",
    "                      \n",
    "        boxes, probs = self.mtcnn.detect(frames)\n",
    "        \n",
    "#         print(boxes)\n",
    "#         print(probs)\n",
    "#         return (boxes, probs)\n",
    "        \n",
    "        faces = []\n",
    "        faces_lag = []\n",
    "        frame_boxes = None\n",
    "        idx = 0\n",
    "        original_length = int(len(frames_original)/2) if double_frames else len(frames_original) \n",
    "        for i in range(original_length):\n",
    "            frame_boxes = boxes[i]\n",
    "            if frame_boxes is None:\n",
    "                pass\n",
    "            elif len(frame_boxes) > 1:\n",
    "                return []\n",
    "            else:\n",
    "                frame = frames_original[idx]\n",
    "                for box in frame_boxes:\n",
    "                    faces.append(np.array(frame.crop(box/self.resize)))\n",
    "                    if double_frames:\n",
    "                        frame = frames_original[idx+1]\n",
    "                        faces_lag.append(np.array(frame.crop(box/self.resize)))\n",
    "                if double_frames:\n",
    "                    idx = idx + 1\n",
    "                idx = idx + 1\n",
    "        faces.extend(faces_lag)\n",
    "        \n",
    "        return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.451099Z",
     "start_time": "2020-02-09T07:13:05.375Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def return_all_video_paths_v2(train_path = '../deepfake_train_full'):\n",
    "    train_folders = [os.path.join(train_path, x) for x in os.listdir(train_path) if x[-1] != 'p']\n",
    "    train_video_folders = [os.path.join(x, os.listdir(x)[0]) for x in train_folders]\n",
    "\n",
    "    all_videos = {}\n",
    "    for train_video_folder in train_video_folders:\n",
    "        with open(join(train_video_folder, 'metadata.json')) as json_file:\n",
    "            metadata = json.load(json_file)\n",
    "        all_videos[train_video_folder] = []\n",
    "        for key in metadata:\n",
    "            ann = metadata[key]\n",
    "            if ann['label'] == 'FAKE':\n",
    "                all_videos[train_video_folder].append({\n",
    "                    'FAKE': join(train_video_folder, key),\n",
    "                    'REAL': join(train_video_folder, ann['original'])}\n",
    "                )\n",
    "\n",
    "    return all_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.452098Z",
     "start_time": "2020-02-09T07:13:05.378Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GetFaces:\n",
    "    \n",
    "    def __init__(self, fast_mtcnn):\n",
    "        self.fast_mtcnn = fast_mtcnn\n",
    "        \n",
    "    @staticmethod\n",
    "    def open_video(path):\n",
    "        v_cap = FileVideoStream(path).start()\n",
    "        v_len = int(v_cap.stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        return (v_cap, v_len)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_frames_from_video(v_cap, v_len, double_frames=False, nr_of_frames=48, offset=5):\n",
    "        frames = []\n",
    "        for j in range(v_len):\n",
    "            frame = v_cap.read()\n",
    "            if frame is not None:\n",
    "                if j%offset == 0 or (((j+int(offset/2))%offset == 0) and double_frames):\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame = Image.fromarray(frame)\n",
    "                    frames.append(frame)\n",
    "            else:\n",
    "                break\n",
    "            if len(frames) >= nr_of_frames:\n",
    "                if double_frames and len(frames) >= nr_of_frames*2:\n",
    "                    break\n",
    "                elif not double_frames:\n",
    "                    break\n",
    "                \n",
    "        return frames\n",
    "    \n",
    "    def detect_faces(self, frames, double_frames=False):\n",
    "        return self.fast_mtcnn(frames, double_frames)\n",
    "    \n",
    "    def resize_faces(self, faces, augment):\n",
    "        args = {}\n",
    "        new_shape = (224,224)\n",
    "        resized_faces = []\n",
    "        for face in faces:\n",
    "            img = face\n",
    "            if np.argmax(img.shape[:2]) == 0:\n",
    "                args['height'] = new_shape[0]\n",
    "            else:\n",
    "                args['width'] = new_shape[1]\n",
    "            try:\n",
    "                resized_img = imutils.resize(img, **args)\n",
    "                if augment:\n",
    "                    resized_img = self.augment_img(resized_img)\n",
    "                \n",
    "                if np.argmax(img.shape[:2]) == 0:\n",
    "                    diff = new_shape[1] - resized_img.shape[1]\n",
    "                    resized_img = np.pad(resized_img, ((0,0), (0,diff), (0,0)), 'constant', constant_values=0)\n",
    "                else:\n",
    "                    diff = new_shape[0] - resized_img.shape[0]\n",
    "                    resized_img = np.pad(resized_img, ((0,diff), (0,0), (0,0)), 'constant', constant_values=0)\n",
    "                resized_faces.append(resized_img)\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "        return resized_faces\n",
    "    \n",
    "    def create_batch(self, resized_fake_faces, resized_real_faces, \\\n",
    "                     batch_size, video_annotation):\n",
    "#         batch_fake = []\n",
    "#         batch_real = []\n",
    "        batch = []\n",
    "#         for i in range(m*2):\n",
    "        for i in range(batch_size):\n",
    "            s = int((batch_size/2)*i)\n",
    "            e = int((batch_size/2)*(i+1))\n",
    "            if s >= len(resized_real_faces) or s >= len(resized_fake_faces):\n",
    "                break\n",
    "            real = resized_real_faces[s:e]\n",
    "            fake = resized_fake_faces[s:e]\n",
    "            if len(real) > 0 and len(fake) > 0:\n",
    "                X = np.array(fake + real) / 255\n",
    "                Y = np.concatenate((np.ones(len(resized_fake_faces[s:e])), np.zeros(len(resized_real_faces[s:e]))))\n",
    "                if len(X) > 0 and len(Y) > 0:\n",
    "#                     if augment:\n",
    "#                         temp = []\n",
    "#                         for x in X:\n",
    "#                             temp.append(self.augment_img(x))\n",
    "#                         X = np.array(temp)\n",
    "                    X = X.reshape((X.shape[0],3,224,224))\n",
    "                    Y = np.expand_dims(np.array(Y), axis=1)\n",
    "                    batch.append((X, Y))\n",
    "                else:\n",
    "                    continue\n",
    "    #                 print('---')\n",
    "    #                 print('0 faces from:', video_annotation)\n",
    "                \n",
    "        return batch\n",
    "    \n",
    "    def augment_img(self, img):\n",
    "        if random.random() > 0.5:\n",
    "            img = np.flip(img, axis=1)\n",
    "            \n",
    "        return img\n",
    "    \n",
    "    def process_functions(self, video_key, nr_of_frames=48, frame_offset=5, double_frames=False):\n",
    "        video, video_len = self.open_video(video_key)\n",
    "        frames = self.get_frames_from_video(video, video_len, nr_of_frames=nr_of_frames, \\\n",
    "                                            offset=frame_offset, double_frames=double_frames)\n",
    "        faces = self.detect_faces(frames, double_frames)\n",
    "        resized_faces = self.resize_faces(faces, augment=False)\n",
    "        \n",
    "        return resized_faces\n",
    "    \n",
    "    def get_resized_faces(self, video_annotation, double_frames=False, batch_size=16, nr_of_frames=48):\n",
    "        start = time.time()\n",
    "        \n",
    "        resized_fake_faces = self.process_functions(video_annotation['FAKE'], nr_of_frames, double_frames=double_frames)\n",
    "        resized_real_faces = self.process_functions(video_annotation['REAL'], nr_of_frames, double_frames=double_frames)\n",
    "        \n",
    "        batches = self.create_batch(resized_fake_faces, resized_real_faces, \\\n",
    "                     batch_size, video_annotation)\n",
    "    \n",
    "        return batches\n",
    "    \n",
    "    def insert_data_to_list(self, shared_list, all_videos_annotations, batch_size=16, max_len_shared_list=100):\n",
    "        keys = all_videos_annotations.keys()\n",
    "        while True:\n",
    "            ones = np.ones(batch_size)\n",
    "            zeros = np.zeros(batch_size)\n",
    "            \n",
    "            # Random folder and video each iteration\n",
    "            key = random.choice(list(keys))\n",
    "            video_annotation = random.choice(all_videos_annotations[key])\n",
    "            \n",
    "#             for key in self.all_videos_annotations:\n",
    "#                 for video_annotation in self.all_videos_annotations[key]:\n",
    "#             print(video_annotation)\n",
    "            try:\n",
    "                batches = self.get_resized_faces(video_annotation, double_frames=True)\n",
    "                for X, Y in batches:\n",
    "                    shared_list.append((X, Y))\n",
    "                random.shuffle(shared_list)\n",
    "            except Exception as e:\n",
    "#                         continue\n",
    "                print('-----')\n",
    "                print(e)\n",
    "                print(traceback.format_exc())\n",
    "                print(video_annotation)\n",
    "            # Check if list already full\n",
    "            while len(shared_list) > max_len_shared_list:\n",
    "                time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.453098Z",
     "start_time": "2020-02-09T07:13:05.380Z"
    }
   },
   "outputs": [],
   "source": [
    "all_videos_annotations = return_all_video_paths_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.454121Z",
     "start_time": "2020-02-09T07:13:05.382Z"
    }
   },
   "outputs": [],
   "source": [
    "fast_mtcnn = FastMTCNN(\n",
    "    resize=0.25,\n",
    "    margin=14,\n",
    "    factor=0.6,\n",
    "    keep_all=True,\n",
    "    device=device,\n",
    "    thresholds=[0.95,0.95,0.95]\n",
    ")\n",
    "\n",
    "# fast_mtcnn = FastMTCNN(\n",
    "#     resize=0.25,\n",
    "#     margin=0,\n",
    "#     select_largest=False,\n",
    "# #     factor=0.6,\n",
    "# #     keep_all=False,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.454121Z",
     "start_time": "2020-02-09T07:13:05.385Z"
    }
   },
   "outputs": [],
   "source": [
    "get_faces = GetFaces(fast_mtcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.455098Z",
     "start_time": "2020-02-09T07:13:05.387Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_list = []\n",
    "get_faces = GetFaces(fast_mtcnn)\n",
    "val_split = int(len(all_videos_annotations)*0.9)\n",
    "train_all_videos_annotations = {key: all_videos_annotations[key] for key in list(all_videos_annotations.keys())[:val_split]}\n",
    "val_all_videos_annotations = {key: all_videos_annotations[key] for key in list(all_videos_annotations.keys())[val_split:]}\n",
    "p_train = threading.Thread(target=get_faces.insert_data_to_list, \\\n",
    "            args=(train_list, train_all_videos_annotations,))\n",
    "p_train.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.456098Z",
     "start_time": "2020-02-09T07:13:05.390Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.456098Z",
     "start_time": "2020-02-09T07:13:05.392Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.457121Z",
     "start_time": "2020-02-09T07:13:05.395Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(shared_list):\n",
    "    while True:\n",
    "        if len(shared_list) > 50:\n",
    "            yield shared_list.pop(0)\n",
    "        else:\n",
    "            time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.458098Z",
     "start_time": "2020-02-09T07:13:05.397Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_train = generator(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.458098Z",
     "start_time": "2020-02-09T07:13:05.399Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x, y in gen_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.459121Z",
     "start_time": "2020-02-09T07:13:05.402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in x:\n",
    "    plt.imshow(img.reshape((224,224,3)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.460098Z",
     "start_time": "2020-02-09T07:13:05.403Z"
    }
   },
   "outputs": [],
   "source": [
    "frame = x[3].reshape((224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.461098Z",
     "start_time": "2020-02-09T07:13:05.405Z"
    }
   },
   "outputs": [],
   "source": [
    "img = np.flip([frame], axis=2)\n",
    "plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.461098Z",
     "start_time": "2020-02-09T07:13:05.407Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.462097Z",
     "start_time": "2020-02-09T07:13:05.411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet101(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# model_ft.fc = nn.Sigmoid()\n",
    "# model_ft.add_module('last_sigmoid', nn.Sigmoid())\n",
    "model_ft.fc = nn.Sequential(nn.Linear(num_ftrs, 1), nn.Sigmoid())\n",
    "# model_ft = nn.Sequential(model_ft, nn.Sigmoid())\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.0002, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) \n",
    "lr_sch = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor=0.3, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.463123Z",
     "start_time": "2020-02-09T07:13:05.414Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.463123Z",
     "start_time": "2020-02-09T07:13:05.417Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, generator, \n",
    "                num_epochs=25, steps_per_epoch=500):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 9999\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        step = 0\n",
    "        for inputs, labels in generator:\n",
    "            inputs = torch.from_numpy(inputs).float().to(device)\n",
    "            labels = torch.from_numpy(labels).float().to(device)\n",
    "\n",
    "            step = step + 1\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "#                 loss = criterion(outputs) - labels\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            print(\"Step: {} - Loss {}\".format(step, running_loss / step), end=\"\\r\")\n",
    "#             print(running_loss / step)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            if step > steps_per_epoch:\n",
    "                break\n",
    "        scheduler.step(running_loss)\n",
    "\n",
    "        epoch_loss = running_loss / steps_per_epoch\n",
    "        epoch_acc = running_corrects / steps_per_epoch\n",
    "\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            epoch_loss, epoch_acc))\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_loss < best_loss:\n",
    "#             model.save_state_dict('mytraining.pt')\n",
    "            torch.save(model, 'mytraining_full_3.pth')\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T07:13:05.464099Z",
     "start_time": "2020-02-09T07:13:05.419Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, lr_sch,\n",
    "                       gen_train,\n",
    "                       num_epochs=200,\n",
    "                       steps_per_epoch=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:38:48.225857Z",
     "start_time": "2020-02-05T06:44:15.889715Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "Loss: 9.6770 Acc: 127.0000343595277\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "Loss: 9.7428 Acc: 126.0000755617904\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "Loss: 9.9018 Acc: 126.0000246129376\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "Loss: 9.8084 Acc: 127.0000616726974\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "Loss: 9.9175 Acc: 127.0000045777875\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "Loss: 9.7198 Acc: 126.0000140479506\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "----- 1052 - Loss 9.844530181853036\n",
      "need at least one array to stack\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 136, in insert_data_to_list\n",
      "    batches = self.get_resized_faces(video_annotation, double_frames=True)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 114, in get_resized_faces\n",
      "    resized_fake_faces = self.process_functions(video_annotation['FAKE'], nr_of_frames, double_frames=double_frames)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 106, in process_functions\n",
      "    faces = self.detect_faces(frames, double_frames)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 34, in detect_faces\n",
      "    return self.fast_mtcnn(frames, double_frames)\n",
      "  File \"<ipython-input-4-140578ad711d>\", line 25, in __call__\n",
      "    boxes, probs = self.mtcnn.detect(frames)\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\", line 347, in detect\n",
      "    self.device\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py\", line 16, in detect_face\n",
      "    imgs_np = np.stack([np.uint8(img) for img in imgs])\n",
      "  File \"<__array_function__ internals>\", line 6, in stack\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\shape_base.py\", line 422, in stack\n",
      "    raise ValueError('need at least one array to stack')\n",
      "ValueError: need at least one array to stack\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_18\\\\dfdc_train_part_18\\\\pvohowzowy.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_18\\\\dfdc_train_part_18\\\\bgcvbayfhn.mp4'}\n",
      "Loss: 9.8097 Acc: 127.0000231765056\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "Loss: 9.8316 Acc: 126.0000389552226\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "Loss: 9.8864 Acc: 127.0000674827968\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "Loss: 9.7685 Acc: 126.0000866913029\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "Loss: 9.8121 Acc: 126.0000847039565\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "Loss: 9.5307 Acc: 127.0000855729258\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "Loss: 9.7877 Acc: 126.0000316835647\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "Loss: 9.8698 Acc: 127.0000412252618\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "Loss: 9.7229 Acc: 126.0000829086229\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "Loss: 9.7794 Acc: 126.0000232626687\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "Loss: 9.8412 Acc: 126.0000845340492\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "Loss: 9.9032 Acc: 126.0000841461722\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "----- 1027 - Loss 9.909152658853935\n",
      "list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 139, in insert_data_to_list\n",
      "    random.shuffle(shared_list)\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\random.py\", line 277, in shuffle\n",
      "    x[i], x[j] = x[j], x[i]\n",
      "IndexError: list index out of range\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_21\\\\dfdc_train_part_21\\\\cjxshjppnd.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_21\\\\dfdc_train_part_21\\\\ptwlgtvhmt.mp4'}\n",
      "Loss: 9.7707 Acc: 127.0000036066155\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "Loss: 9.8163 Acc: 126.0000075046236\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "Loss: 9.7181 Acc: 126.0000076069802\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "Loss: 9.6287 Acc: 126.0000277371318\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "Loss: 9.7374 Acc: 126.0000196794443\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "Loss: 9.6979 Acc: 126.0000369809239\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "Loss: 9.8243 Acc: 126.0000589364192\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "Step: 579 - Loss 9.598575304415148\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-98f860d127fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                        \u001b[0mgen_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                        \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                        steps_per_epoch=2000)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-6a4dd1f909a7>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, generator, num_epochs, steps_per_epoch)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Iterate over data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-dc0eca75c460>\u001b[0m in \u001b[0;36mgenerator\u001b[1;34m(shared_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mshared_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = torch.load('mytraining_full_2.pth')\n",
    "model_ft2 = train_model(model_ft, criterion, optimizer_ft, lr_sch,\n",
    "                       gen_train,\n",
    "                       num_epochs=200,\n",
    "                       steps_per_epoch=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:32:47.530054Z",
     "start_time": "2020-02-04T05:02:41.740657Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "Loss: 9.6559 Acc: 126.0000327526551\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "Loss: 9.7508 Acc: 127.0000320994147\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "Loss: 9.6673 Acc: 126.0000108060417\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "Loss: 9.7608 Acc: 126.0000061871283\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "Loss: 9.7372 Acc: 126.0000163245012\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "Loss: 9.7060 Acc: 127.0000143164387\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "Loss: 9.8739 Acc: 126.0000670197437\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "Loss: 10.0110 Acc: 127.0000856854983\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "Loss: 9.6635 Acc: 126.0000943465534\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "----- 1111 - Loss 9.656533383014072\n",
      "list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 139, in insert_data_to_list\n",
      "    random.shuffle(shared_list)\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\random.py\", line 277, in shuffle\n",
      "    x[i], x[j] = x[j], x[i]\n",
      "IndexError: list index out of range\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_13\\\\dfdc_train_part_13\\\\sherosbvvz.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_13\\\\dfdc_train_part_13\\\\nddepqnpvd.mp4'}\n",
      "Loss: 9.6528 Acc: 126.0000992425228\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "----- 93 - Loss 10.273476077664283\n",
      "list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 139, in insert_data_to_list\n",
      "    random.shuffle(shared_list)\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\random.py\", line 277, in shuffle\n",
      "    x[i], x[j] = x[j], x[i]\n",
      "IndexError: list index out of range\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_15\\\\dfdc_train_part_15\\\\verpifrngx.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_15\\\\dfdc_train_part_15\\\\uebjocpkna.mp4'}\n",
      "Loss: 9.7986 Acc: 127.00004495602738\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "Loss: 9.8434 Acc: 127.0000289101921\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "Loss: 9.5626 Acc: 126.0000172103944\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "Loss: 9.7915 Acc: 127.0000680191069\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "Loss: 9.6311 Acc: 126.0000363019334\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "Loss: 9.6529 Acc: 126.0000053381739\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "Loss: 9.7054 Acc: 126.0000732607684\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "Loss: 9.6271 Acc: 127.0000147425486\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "Loss: 9.7988 Acc: 126.0000316312052\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "----- 913 - Loss 9.7368579618510384\n",
      "cannot reshape array of size 16 into shape (16,3,224,224)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 136, in insert_data_to_list\n",
      "    batches = self.get_resized_faces(video_annotation, double_frames=True)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 118, in get_resized_faces\n",
      "    batch_size, video_annotation)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 86, in create_batch\n",
      "    X = X.reshape((X.shape[0],3,224,224))\n",
      "ValueError: cannot reshape array of size 16 into shape (16,3,224,224)\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_15\\\\dfdc_train_part_15\\\\djbmrrjelb.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_15\\\\dfdc_train_part_15\\\\odotpnxgou.mp4'}\n",
      "Loss: 9.6703 Acc: 127.0000891400742\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "Loss: 9.8858 Acc: 126.0000449305056\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "Loss: 9.9021 Acc: 126.00007617561413\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "Step: 366 - Loss 9.746251112450667\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-98f860d127fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                        \u001b[0mgen_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                        \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                        steps_per_epoch=2000)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-6a4dd1f909a7>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, generator, num_epochs, steps_per_epoch)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Iterate over data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-dc0eca75c460>\u001b[0m in \u001b[0;36mgenerator\u001b[1;34m(shared_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mshared_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = torch.load('mytraining_full_2.pth')\n",
    "model_ft2 = train_model(model_ft, criterion, optimizer_ft, lr_sch,\n",
    "                       gen_train,\n",
    "                       num_epochs=200,\n",
    "                       steps_per_epoch=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-03T03:14:59.296Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "Loss: 10.5068 Acc: 127.0000904371126\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "Loss: 10.0317 Acc: 127.0000906314531\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "----- 1832 - Loss 9.774019267849265\n",
      "need at least one array to stack\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 136, in insert_data_to_list\n",
      "    batches = self.get_resized_faces(video_annotation, double_frames=True)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 114, in get_resized_faces\n",
      "    resized_fake_faces = self.process_functions(video_annotation['FAKE'], nr_of_frames, double_frames=double_frames)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 106, in process_functions\n",
      "    faces = self.detect_faces(frames, double_frames)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 34, in detect_faces\n",
      "    return self.fast_mtcnn(frames, double_frames)\n",
      "  File \"<ipython-input-4-140578ad711d>\", line 25, in __call__\n",
      "    boxes, probs = self.mtcnn.detect(frames)\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\", line 347, in detect\n",
      "    self.device\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py\", line 16, in detect_face\n",
      "    imgs_np = np.stack([np.uint8(img) for img in imgs])\n",
      "  File \"<__array_function__ internals>\", line 6, in stack\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\shape_base.py\", line 422, in stack\n",
      "    raise ValueError('need at least one array to stack')\n",
      "ValueError: need at least one array to stack\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_18\\\\dfdc_train_part_18\\\\wipjitfmta.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_18\\\\dfdc_train_part_18\\\\suybcasguz.mp4'}\n",
      "Loss: 9.7678 Acc: 127.0000916940713\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "Loss: 9.7508 Acc: 126.0000091675832\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "----- 856 - Loss 9.550623604488154\n",
      "cannot reshape array of size 16 into shape (16,3,224,224)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 136, in insert_data_to_list\n",
      "    batches = self.get_resized_faces(video_annotation, double_frames=True)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 118, in get_resized_faces\n",
      "    batch_size, video_annotation)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 86, in create_batch\n",
      "    X = X.reshape((X.shape[0],3,224,224))\n",
      "ValueError: cannot reshape array of size 16 into shape (16,3,224,224)\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_15\\\\dfdc_train_part_15\\\\djbmrrjelb.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_15\\\\dfdc_train_part_15\\\\odotpnxgou.mp4'}\n",
      "Loss: 9.5977 Acc: 126.0000278766617\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "Loss: 9.6423 Acc: 127.0000698705923\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "Loss: 9.7076 Acc: 126.0000953146866\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "Loss: 9.5725 Acc: 126.0000961382571\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "Loss: 9.6027 Acc: 127.0000196934564\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "Loss: 9.5470 Acc: 126.0000560671227\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "Loss: 9.4261 Acc: 127.0000407342502\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "Loss: 9.4834 Acc: 126.0000780469378\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "Loss: 9.5436 Acc: 127.0000476278193\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "Loss: 9.6115 Acc: 126.0000481633794\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "----- 523 - Loss 9.815964884557413\n",
      "cannot reshape array of size 16 into shape (16,3,224,224)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 136, in insert_data_to_list\n",
      "    batches = self.get_resized_faces(video_annotation, double_frames=True)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 118, in get_resized_faces\n",
      "    batch_size, video_annotation)\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 86, in create_batch\n",
      "    X = X.reshape((X.shape[0],3,224,224))\n",
      "ValueError: cannot reshape array of size 16 into shape (16,3,224,224)\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_29\\\\dfdc_train_part_29\\\\bwrwssyltx.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_29\\\\dfdc_train_part_29\\\\nviqscyhuy.mp4'}\n",
      "Loss: 9.7424 Acc: 126.0000790653156\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "Loss: 9.4181 Acc: 127.0000107233147\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "----- 1400 - Loss 9.292409205394131\n",
      "list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 139, in insert_data_to_list\n",
      "    random.shuffle(shared_list)\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\random.py\", line 277, in shuffle\n",
      "    x[i], x[j] = x[j], x[i]\n",
      "IndexError: list index out of range\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_05\\\\dfdc_train_part_5\\\\ebyhzsyxbe.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_05\\\\dfdc_train_part_5\\\\ncmtepztza.mp4'}\n",
      "Loss: 9.3209 Acc: 126.0000035012704\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "Loss: 9.4777 Acc: 126.0000264521723\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "Loss: 9.3042 Acc: 127.0000915941983\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "Loss: 9.2797 Acc: 126.0000027515267\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "Loss: 9.1751 Acc: 126.0000334322957\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "Loss: 9.2531 Acc: 126.0000242119961\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "Loss: 9.4338 Acc: 126.0000542614655\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "----- 1219 - Loss 9.393805335837373\n",
      "list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-c9b2f71155d1>\", line 139, in insert_data_to_list\n",
      "    random.shuffle(shared_list)\n",
      "  File \"C:\\Users\\Anti\\Anaconda3\\envs\\torch\\lib\\random.py\", line 277, in shuffle\n",
      "    x[i], x[j] = x[j], x[i]\n",
      "IndexError: list index out of range\n",
      "\n",
      "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_38\\\\dfdc_train_part_38\\\\egolbmkdql.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_38\\\\dfdc_train_part_38\\\\qhbbvqwrlf.mp4'}\n",
      "Loss: 9.3644 Acc: 126.0000283226626\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "Step: 910 - Loss 9.281377416652637\r"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, lr_sch,\n",
    "                       gen_train,\n",
    "                       num_epochs=200,\n",
    "                       steps_per_epoch=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.389Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x, y in gen_train:\n",
    "    if i > 4:\n",
    "        break\n",
    "    i = i + 1\n",
    "    idx = 0\n",
    "    print(y[idx])\n",
    "    plt.imshow(x[idx].reshape((224,224,3)))\n",
    "    plt.show()\n",
    "    \n",
    "    idx = 9\n",
    "    print(y[idx])\n",
    "    plt.imshow(x[idx].reshape((224,224,3)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.393Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(y[idx])\n",
    "plt.imshow(x[idx].reshape((224,224,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T02:34:28.587860Z",
     "start_time": "2020-02-02T02:34:28.582861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FAKE': '../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\owxbbpjpch.mp4',\n",
       " 'REAL': '../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\wynotylpnm.mp4'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_videos_annotations['../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T02:34:35.702855Z",
     "start_time": "2020-02-02T02:34:34.246857Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet101(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Sequential(nn.Linear(num_ftrs, 1), nn.Sigmoid())\n",
    "\n",
    "model_ft = model_ft.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T02:34:31.279860Z",
     "start_time": "2020-02-02T02:34:30.916861Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.0.weight\", \"fc.0.bias\". \n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7282971e5e59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_ft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mytraining.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    828\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 830\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    831\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.0.weight\", \"fc.0.bias\". \n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". "
     ]
    }
   ],
   "source": [
    "model_ft.load_state_dict(torch.load('mytraining.pt'))\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T02:34:40.557856Z",
     "start_time": "2020-02-02T02:34:39.126856Z"
    }
   },
   "outputs": [],
   "source": [
    "# resized_faces = get_faces.process_functions('../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\owxbbpjpch.mp4', \\\n",
    "#                                             nr_of_frames=10,\\\n",
    "#                                             frame_offset=20)\n",
    "resized_faces = get_faces.process_functions('../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\wynotylpnm.mp4', \\\n",
    "                                            nr_of_frames=10,\\\n",
    "                                            frame_offset=20)\n",
    "X = np.array(resized_faces)\n",
    "X = X.reshape((X.shape[0],3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T02:34:41.243859Z",
     "start_time": "2020-02-02T02:34:41.127860Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.set_grad_enabled(True):\n",
    "    inputs = torch.from_numpy(X).float().to(device)\n",
    "    outputs = model_ft(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T02:34:41.742861Z",
     "start_time": "2020-02-02T02:34:41.726858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5907],\n",
       "        [0.5313],\n",
       "        [0.6162],\n",
       "        [0.5726],\n",
       "        [0.6020],\n",
       "        [0.5330],\n",
       "        [0.6194],\n",
       "        [0.5655],\n",
       "        [0.5579],\n",
       "        [0.5624]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.414Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for x, y in train_gen:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.428Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(y[idx])\n",
    "plt.imshow(x[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.430Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = 9\n",
    "print(y[idx])\n",
    "plt.imshow(x[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.434Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generator(shared_list, batch_size):\n",
    "    while True:\n",
    "        if len(shared_list) > batch_size:\n",
    "            X = []\n",
    "            Y = []\n",
    "            random.shuffle(shared_list)\n",
    "            for i in range(batch_size):\n",
    "                x, y = shared_list.pop()\n",
    "\n",
    "                X.append(x/255)\n",
    "                Y.append(y)\n",
    "            yield (np.array(X).reshape((batch_size,3,224,224)), \\\n",
    "                   np.expand_dims(np.array(Y), axis=1))\n",
    "        else:\n",
    "            time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.442Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch = get_faces.get_resized_faces({'FAKE': '../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\ohaqlzfnuv.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\sttnfyptum.mp4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(batch[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(batch[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.448Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxes, probs = get_faces.get_resized_faces({'FAKE': '../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\owxbbpjpch.mp4', 'REAL': '../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\wynotylpnm.mp4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.450Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.451Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.453Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(boxes)):\n",
    "    if boxes[i] is None:\n",
    "        print('None', i)\n",
    "    elif len(boxes[i]) == 2:\n",
    "#         print(len(boxes[i]))\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T01:21:36.671229Z",
     "start_time": "2020-01-29T01:21:36.667228Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "video_annotation ={'FAKE': '../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\ohaqlzfnuv.mp4', \n",
    "                   'REAL': '../deepfake_train_full\\\\dfdc_train_part_00\\\\dfdc_train_part_0\\\\sttnfyptum.mp4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.458Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "batch_fake, batch_real = get_faces.get_resized_faces(video_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.460Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(batch_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.463Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(batch_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.464Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(batch_fake[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(batch_real[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.471Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.concatenate((np.ones(3), np.zeros(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "probs[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.480Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(MTCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.483Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "probs[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.488Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array(train_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.490Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_list = []\n",
    "batch_fake, batch_real = get_faces.insert_data_to_list(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.493Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.array(train_list[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.501Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.505Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_list[0][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "split_batch = 8\n",
    "n = 1\n",
    "len(batch_fake[i][split_batch*n:split_batch*(n+1)] + batch_real[i][split_batch*n:split_batch*(n+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.513Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generator(shared_list, batch_size):\n",
    "    while True:\n",
    "        if len(shared_list) > batch_size:\n",
    "            X = []\n",
    "            Y = []\n",
    "            random.shuffle(shared_list)\n",
    "            for i in range(batch_size):\n",
    "                x, y = shared_list.pop()\n",
    "\n",
    "                X.append(x/255)\n",
    "                Y.append(y)\n",
    "            yield (np.array(X).reshape((batch_size,3,224,224)), \\\n",
    "                   np.expand_dims(np.array(Y), axis=1))\n",
    "        else:\n",
    "            time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.515Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_train = generator(train_list, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.516Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.519Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet101(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.522Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, generator, \n",
    "                num_epochs=25, steps_per_epoch=500):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 9999\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        step = 0\n",
    "        for inputs, labels in generator:\n",
    "            inputs = torch.from_numpy(inputs).float().to(device)\n",
    "            labels = torch.from_numpy(labels).float().to(device)\n",
    "\n",
    "            step = step + 1\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "#                 loss = criterion(outputs) - labels\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            print(\"Step: {} - Loss {}\".format(step, running_loss / step), end=\"\\r\")\n",
    "#             print(running_loss / step)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            if step > steps_per_epoch:\n",
    "                break\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / steps_per_epoch\n",
    "        epoch_acc = running_corrects / steps_per_epoch\n",
    "\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            epoch_loss, epoch_acc))\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_loss < best_loss:\n",
    "#             model.save_state_dict('mytraining.pt')\n",
    "            torch.save(model.state_dict(), 'mytraining.pt')\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.524Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       gen_train,\n",
    "                       num_epochs=50,\n",
    "                       steps_per_epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.526Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for X, Y in gen_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.528Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.expand_dims(Y, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.539Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for x in range(10):\n",
    "    print(\"Progress {}\".format(x / 10), end=\"\\r\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.541Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = applications.mobilenet_v2.MobileNetV2(include_top=True, weights='imagenet', input_shape=(224, 224, 3))\n",
    "x = model.output\n",
    "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "model_final = Model(inputs = model.input, outputs = predictions)\n",
    "model_final.compile(loss = \"mean_squared_logarithmic_error\", \\\n",
    "                    optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"MobileNetV2_retrain.h5\", monitor='accuracy', verbose=1, \\\n",
    "                             save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=7, verbose=0, \\\n",
    "                              mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.549Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_final.fit_generator(\n",
    "  gen_train,\n",
    "  steps_per_epoch = 500,\n",
    "  epochs = 1000,\n",
    "#   validation_data = gen_val,\n",
    "#   validation_steps = 5,\n",
    "  callbacks = [reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.551Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_final.fit_generator(\n",
    "  gen_train,\n",
    "  steps_per_epoch = 500,\n",
    "  epochs = 1000,\n",
    "#   validation_data = gen_val,\n",
    "#   validation_steps = 5,\n",
    "  callbacks = [reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.553Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_final.fit_generator(\n",
    "  gen_train,\n",
    "  steps_per_epoch = 500,\n",
    "  epochs = 1000,\n",
    "#   validation_data = gen_val,\n",
    "#   validation_steps = 5,\n",
    "  callbacks = [reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.555Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_final.fit_generator(\n",
    "  gen_train,\n",
    "  steps_per_epoch = 500,\n",
    "  epochs = 1000,\n",
    "#   validation_data = gen_val,\n",
    "#   validation_steps = 5,\n",
    "  callbacks = [reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.557Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(tf_build_info.cuda_version_number)\n",
    "# 9.0 in v1.10.0\n",
    "print(tf_build_info.cudnn_version_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for X, Y in gen_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.560Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.561Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.565Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_final.fit(X,np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.567Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ctypes\n",
    "ctypes.WinDLL(\"cudnn64_7.dll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.568Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.570Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extract audio from video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Speech recogniziton. SLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.575Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.577Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "command2mp3 = 'ffmpeg -i ../deepfake_train_full\\\\dfdc_train_part_46\\\\dfdc_train_part_46\\\\aqsgzoxyok.mp4 \\\n",
    "    ../deepfake_train_full_audio\\\\aqsgzoxyok.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.580Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.system(command2mp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.583Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.585Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with sr.AudioFile('../deepfake_train_full_audio\\\\aqsgzoxyok.wav') as source:\n",
    "    audio = r.record(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.587Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Sphinx thinks you said \" + r.recognize_sphinx(audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.589Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "audio = sr.AudioFile('../deepfake_train_full_audio\\\\aqsgzoxyok.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.593Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "audio = r.record(source, duration=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.595Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(r.recognize_google(audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Plot signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.597Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "command2mp3 = 'ffmpeg -i ../deepfake_train_full\\\\dfdc_train_part_26\\\\dfdc_train_part_26\\\\gpdtoamvkz.mp4 \\\n",
    "    ../deepfake_train_full_audio\\\\gpdtoamvkz.wav'\n",
    "os.system(command2mp3)\n",
    "\n",
    "command2mp3 = 'ffmpeg -i ../deepfake_train_full\\\\dfdc_train_part_26\\\\dfdc_train_part_26\\\\yojgjueqta.mp4 \\\n",
    "    ../deepfake_train_full_audio\\\\yojgjueqta.wav'\n",
    "os.system(command2mp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T02:10:36.599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wave\n",
    "import sys\n",
    "import math\n",
    "\n",
    "\n",
    "spf = wave.open('../deepfake_train_full_audio\\\\gpdtoamvkz.wav')\n",
    "\n",
    "# Extract Raw Audio from Wav File\n",
    "signal = spf.readframes(-1)\n",
    "signal = np.fromstring(signal, \"Int16\")\n",
    "\n",
    "\n",
    "# If Stereo\n",
    "if spf.getnchannels() == 2:\n",
    "    print(\"Just mono files\")\n",
    "    sys.exit(0)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Signal Wave...\")\n",
    "plt.plot(signal)\n",
    "for i in range(int(len(signal)/48000)):\n",
    "    plt.axvline(x=48000*(i+1), color='red')\n",
    "plt.show()\n",
    "\n",
    "spf = wave.open('../deepfake_train_full_audio\\\\yojgjueqta.wav')\n",
    "\n",
    "# Extract Raw Audio from Wav File\n",
    "signal = spf.readframes(-1)\n",
    "signal = np.fromstring(signal, \"Int16\")\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Signal Wave...\")\n",
    "plt.plot(signal)\n",
    "for i in range(int(len(signal)/48000)):\n",
    "    plt.axvline(x=48000*(i+1), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
